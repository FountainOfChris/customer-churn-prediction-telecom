{"cells":[{"source":"![Cartoon of telecom customers](IMG_8811.png)\n","metadata":{},"id":"cdd466f9-a72a-44df-9e00-6926a97a4923","cell_type":"markdown"},{"source":"The telecommunications (telecom) sector in India is rapidly changing, with more and more telecom businesses being created and many customers deciding to switch between providers. \"Churn\" refers to the process where customers or subscribers stop using a company's services or products. Understanding the factors that influence keeping a customer as a client in predicting churn is crucial for telecom companies to enhance their service quality and customer satisfaction. As the data scientist on this project, you aim to explore the intricate dynamics of customer behavior and demographics in the Indian telecom sector in predicting customer churn, utilizing two comprehensive datasets from four major telecom partners: Airtel, Reliance Jio, Vodafone, and BSNL:\n\n- `telecom_demographics.csv` contains information related to Indian customer demographics:\n\n| Variable             | Description                                      |\n|----------------------|--------------------------------------------------|\n| `customer_id `         | Unique identifier for each customer.             |\n| `telecom_partner `     | The telecom partner associated with the customer.|\n| `gender `              | The gender of the customer.                      |\n| `age `                 | The age of the customer.                         |\n| `state`                | The Indian state in which the customer is located.|\n| `city`                 | The city in which the customer is located.       |\n| `pincode`              | The pincode of the customer's location.          |\n| `registration_event` | When the customer registered with the telecom partner.|\n| `num_dependents`      | The number of dependents (e.g., children) the customer has.|\n| `estimated_salary`     | The customer's estimated salary.                 |\n\n- `telecom_usage` contains information about the usage patterns of Indian customers:\n\n| Variable   | Description                                                  |\n|------------|--------------------------------------------------------------|\n| `customer_id` | Unique identifier for each customer.                         |\n| `calls_made` | The number of calls made by the customer.                    |\n| `sms_sent`   | The number of SMS messages sent by the customer.             |\n| `data_used`  | The amount of data used by the customer.                     |\n| `churn`    | Binary variable indicating whether the customer has churned or not (1 = churned, 0 = not churned).|\n","metadata":{},"id":"dafa483a-e084-4ba8-9236-5c0468364e0d","cell_type":"markdown"},{"source":"# Import libraries and methods/functions\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Start your code here!","metadata":{"executionCancelledAt":null,"executionTime":3776,"lastExecutedAt":1716398012325,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import libraries and methods/functions\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Start your code here!","lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2"},"id":"95efd3c7-a48a-49c2-9df6-36f078de3b38","cell_type":"code","execution_count":1,"outputs":[]},{"source":"## Step 1: Load the CSV files","metadata":{},"cell_type":"markdown","id":"5c5d2524-a2dc-4bd0-90f4-c30e54dbd650"},{"source":"# Load the telecom demographics and usage data\ntelecom_demographics = pd.read_csv('telecom_demographics.csv')\ntelecom_usage = pd.read_csv('telecom_usage.csv')\n\n# Display the first few rows and info of the DataFrames\nprint(\"Telecom Demographics Data:\")\nprint(telecom_demographics.head())\nprint(telecom_demographics.info())\n\nprint(\"\\nTelecom Usage Data:\")\nprint(telecom_usage.head())\nprint(telecom_usage.info())","metadata":{"executionCancelledAt":null,"executionTime":68,"lastExecutedAt":1716398012395,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load the telecom demographics and usage data\ntelecom_demographics = pd.read_csv('telecom_demographics.csv')\ntelecom_usage = pd.read_csv('telecom_usage.csv')\n\n# Display the first few rows and info of the DataFrames\nprint(\"Telecom Demographics Data:\")\nprint(telecom_demographics.head())\nprint(telecom_demographics.info())\n\nprint(\"\\nTelecom Usage Data:\")\nprint(telecom_usage.head())\nprint(telecom_usage.info())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"c9a1413f-0037-42e3-baa5-2d33863ca7be","outputs":[{"output_type":"stream","name":"stdout","text":"Telecom Demographics Data:\n   customer_id telecom_partner  ... num_dependents  estimated_salary\n0        15169          Airtel  ...              4             85979\n1       149207          Airtel  ...              0             69445\n2       148119          Airtel  ...              2             75949\n3       187288    Reliance Jio  ...              3             34272\n4        14016        Vodafone  ...              4             34157\n\n[5 rows x 10 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6500 entries, 0 to 6499\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   customer_id         6500 non-null   int64 \n 1   telecom_partner     6500 non-null   object\n 2   gender              6500 non-null   object\n 3   age                 6500 non-null   int64 \n 4   state               6500 non-null   object\n 5   city                6500 non-null   object\n 6   pincode             6500 non-null   int64 \n 7   registration_event  6500 non-null   object\n 8   num_dependents      6500 non-null   int64 \n 9   estimated_salary    6500 non-null   int64 \ndtypes: int64(5), object(5)\nmemory usage: 507.9+ KB\nNone\n\nTelecom Usage Data:\n   customer_id  calls_made  sms_sent  data_used  churn\n0        15169          75        21       4532      1\n1       149207          35        38        723      1\n2       148119          70        47       4688      1\n3       187288          95        32      10241      1\n4        14016          66        23       5246      1\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6500 entries, 0 to 6499\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype\n---  ------       --------------  -----\n 0   customer_id  6500 non-null   int64\n 1   calls_made   6500 non-null   int64\n 2   sms_sent     6500 non-null   int64\n 3   data_used    6500 non-null   int64\n 4   churn        6500 non-null   int64\ndtypes: int64(5)\nmemory usage: 254.0 KB\nNone\n"}],"execution_count":2},{"source":"# Merge the two DataFrames on 'customer_id'\nchurn_df = pd.merge(telecom_demographics, telecom_usage, on='customer_id')\n\n# Display the first few rows of the merged DataFrame\nprint(churn_df.head())","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1716398012446,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Merge the two DataFrames on 'customer_id'\nchurn_df = pd.merge(telecom_demographics, telecom_usage, on='customer_id')\n\n# Display the first few rows of the merged DataFrame\nprint(churn_df.head())","outputsMetadata":{"0":{"height":185,"type":"stream"}}},"cell_type":"code","id":"28d54aea-f3bc-44c8-a060-e56402088115","outputs":[{"output_type":"stream","name":"stdout","text":"   customer_id telecom_partner gender  ...  sms_sent data_used churn\n0        15169          Airtel      F  ...        21      4532     1\n1       149207          Airtel      F  ...        38       723     1\n2       148119          Airtel      F  ...        47      4688     1\n3       187288    Reliance Jio      M  ...        32     10241     1\n4        14016        Vodafone      M  ...        23      5246     1\n\n[5 rows x 14 columns]\n"}],"execution_count":3},{"source":"## Step 2: Calculate and print the churn rate\n","metadata":{},"cell_type":"markdown","id":"f884a242-ddc8-4bb0-9759-6d96af6b91fb"},{"source":"# Calculate churn rate\nchurn_rate = churn_df['churn'].mean()\nprint(f'Churn Rate: {churn_rate:.2f}')","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1716398012494,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Calculate churn rate\nchurn_rate = churn_df['churn'].mean()\nprint(f'Churn Rate: {churn_rate:.2f}')","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"811e86c8-e700-4af4-b55a-d0662182ebf9","outputs":[{"output_type":"stream","name":"stdout","text":"Churn Rate: 0.20\n"}],"execution_count":4},{"source":"## Step 3: Identify categorical variables\nCategorical variables typically include non-numeric data that can be converted into a numeric format using techniques like one-hot encoding. In our dataset, these variables are likely to include 'telecom_partner', 'gender', 'state', and 'city'.","metadata":{},"cell_type":"markdown","id":"6cbf4ecf-5bce-4867-acbc-a240f3dfe29d"},{"source":"from sklearn.preprocessing import OneHotEncoder\n\n# Identify categorical features\ncategorical_features = ['telecom_partner', 'gender', 'state', 'city']\n\n# Convert 'registration_event' to datetime and then to numerical value (e.g., number of days since the earliest date)\nchurn_df['registration_event'] = pd.to_datetime(churn_df['registration_event'])\nchurn_df['registration_event'] = (churn_df['registration_event'] - churn_df['registration_event'].min()).dt.days\n\n# One-hot encode the categorical features\none_hot_encoder = OneHotEncoder(sparse=False, drop='first')\ncategorical_encoded = one_hot_encoder.fit_transform(churn_df[categorical_features])\n\n# Convert the encoded categorical features into a DataFrame\ncategorical_encoded_df = pd.DataFrame(categorical_encoded, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n\n# Drop the original categorical columns and concatenate the encoded columns\nchurn_df = churn_df.drop(columns=categorical_features)\nchurn_df = pd.concat([churn_df, categorical_encoded_df], axis=1)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1716398012546,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.preprocessing import OneHotEncoder\n\n# Identify categorical features\ncategorical_features = ['telecom_partner', 'gender', 'state', 'city']\n\n# Convert 'registration_event' to datetime and then to numerical value (e.g., number of days since the earliest date)\nchurn_df['registration_event'] = pd.to_datetime(churn_df['registration_event'])\nchurn_df['registration_event'] = (churn_df['registration_event'] - churn_df['registration_event'].min()).dt.days\n\n# One-hot encode the categorical features\none_hot_encoder = OneHotEncoder(sparse=False, drop='first')\ncategorical_encoded = one_hot_encoder.fit_transform(churn_df[categorical_features])\n\n# Convert the encoded categorical features into a DataFrame\ncategorical_encoded_df = pd.DataFrame(categorical_encoded, columns=one_hot_encoder.get_feature_names_out(categorical_features))\n\n# Drop the original categorical columns and concatenate the encoded columns\nchurn_df = churn_df.drop(columns=categorical_features)\nchurn_df = pd.concat([churn_df, categorical_encoded_df], axis=1)"},"cell_type":"code","id":"2e0adbd2-0604-44f0-bb38-45632134d68a","outputs":[],"execution_count":5},{"source":"## Step 5: Feature scaling","metadata":{},"cell_type":"markdown","id":"8a8bc7cd-c254-4a7e-9bcc-e4ca9f5f98d4"},{"source":"# Define the features to be scaled\nfeatures_to_scale = ['age', 'num_dependents', 'estimated_salary', 'calls_made', 'sms_sent', 'data_used']\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Scale the features\nscaled_features = scaler.fit_transform(churn_df[features_to_scale])\n\n# Convert the scaled features into a DataFrame\nscaled_features_df = pd.DataFrame(scaled_features, columns=features_to_scale)\n\n# Drop the original columns and concatenate the scaled columns\nchurn_df = churn_df.drop(columns=features_to_scale)\nchurn_df = pd.concat([churn_df, scaled_features_df], axis=1)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1716398012598,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the features to be scaled\nfeatures_to_scale = ['age', 'num_dependents', 'estimated_salary', 'calls_made', 'sms_sent', 'data_used']\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Scale the features\nscaled_features = scaler.fit_transform(churn_df[features_to_scale])\n\n# Convert the scaled features into a DataFrame\nscaled_features_df = pd.DataFrame(scaled_features, columns=features_to_scale)\n\n# Drop the original columns and concatenate the scaled columns\nchurn_df = churn_df.drop(columns=features_to_scale)\nchurn_df = pd.concat([churn_df, scaled_features_df], axis=1)"},"cell_type":"code","id":"3678fbfe-565e-46a0-9009-98efea52ad7a","outputs":[],"execution_count":6},{"source":"## Step 6: Define features and target variable","metadata":{},"cell_type":"markdown","id":"b1f73fad-3700-49e2-b8a4-80b3ec0b88f3"},{"source":"# Define the target variable\ntarget = 'churn'\n\n# Define the feature set\nfeatures = churn_df.drop(columns=[target])\n\n# Define the target variable\ntarget_variable = churn_df[target]","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1716398012651,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the target variable\ntarget = 'churn'\n\n# Define the feature set\nfeatures = churn_df.drop(columns=[target])\n\n# Define the target variable\ntarget_variable = churn_df[target]"},"cell_type":"code","id":"038b0042-88bb-4ada-8862-9242c98c78fd","outputs":[],"execution_count":7},{"source":"## Step 7: Split the data into training and testing sets","metadata":{},"cell_type":"markdown","id":"45e4f4ee-f18b-4982-9c04-fa6e5d7d319f"},{"source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, target_variable, test_size=0.2, random_state=42)","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1716398012706,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features, target_variable, test_size=0.2, random_state=42)"},"cell_type":"code","id":"5d35974d-6c7e-4da8-ab58-9f46da17d1e2","outputs":[],"execution_count":8},{"source":"## Step 8: Train Logistic Regression and Random Forest models","metadata":{},"cell_type":"markdown","id":"3eee24a6-d211-4923-9c24-608312b7739c"},{"source":"from sklearn.linear_model import LogisticRegression\n\n# Initialize the models\nlogreg_model = LogisticRegression(random_state=42)\nrf_model = RandomForestClassifier(random_state=42)\n\n# Train the models\nlogreg_model.fit(X_train, y_train)\nrf_model.fit(X_train, y_train)\n\n# Predict on the test data\nlogreg_pred = logreg_model.predict(X_test)\nrf_pred = rf_model.predict(X_test)","metadata":{"executionCancelledAt":null,"executionTime":1069,"lastExecutedAt":1716398013775,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.linear_model import LogisticRegression\n\n# Initialize the models\nlogreg_model = LogisticRegression(random_state=42)\nrf_model = RandomForestClassifier(random_state=42)\n\n# Train the models\nlogreg_model.fit(X_train, y_train)\nrf_model.fit(X_train, y_train)\n\n# Predict on the test data\nlogreg_pred = logreg_model.predict(X_test)\nrf_pred = rf_model.predict(X_test)"},"cell_type":"code","id":"1fe4c56b-ab50-4fc1-baad-172ca98e69c5","outputs":[],"execution_count":9},{"source":"## Step 9: Assess the models on test data","metadata":{},"cell_type":"markdown","id":"7ff6bfeb-ac20-4350-8230-498d4906ae54"},{"source":"from sklearn.metrics import accuracy_score\n\n# Calculate accuracy scores\nlogreg_accuracy = accuracy_score(y_test, logreg_pred)\nrf_accuracy = accuracy_score(y_test, rf_pred)\n\n# Determine which model has higher accuracy\nhigher_accuracy = \"LogisticRegression\" if logreg_accuracy > rf_accuracy else \"RandomForest\"\n\n# Print the accuracy scores and the model with higher accuracy\nprint(f'Logistic Regression Accuracy: {logreg_accuracy:.2f}')\nprint(f'Random Forest Accuracy: {rf_accuracy:.2f}')\nprint(f'Higher Accuracy Model: {higher_accuracy}')\n\n# Print classification reports\nprint(\"\\nLogistic Regression Classification Report:\")\nprint(classification_report(y_test, logreg_pred))\n\nprint(\"\\nRandom Forest Classification Report:\")\nprint(classification_report(y_test, rf_pred))\n\n# Print confusion matrices\nprint(\"\\nLogistic Regression Confusion Matrix:\")\nprint(confusion_matrix(y_test, logreg_pred))\n\nprint(\"\\nRandom Forest Confusion Matrix:\")\nprint(confusion_matrix(y_test, rf_pred))","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1716398013826,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.metrics import accuracy_score\n\n# Calculate accuracy scores\nlogreg_accuracy = accuracy_score(y_test, logreg_pred)\nrf_accuracy = accuracy_score(y_test, rf_pred)\n\n# Determine which model has higher accuracy\nhigher_accuracy = \"LogisticRegression\" if logreg_accuracy > rf_accuracy else \"RandomForest\"\n\n# Print the accuracy scores and the model with higher accuracy\nprint(f'Logistic Regression Accuracy: {logreg_accuracy:.2f}')\nprint(f'Random Forest Accuracy: {rf_accuracy:.2f}')\nprint(f'Higher Accuracy Model: {higher_accuracy}')\n\n# Print classification reports\nprint(\"\\nLogistic Regression Classification Report:\")\nprint(classification_report(y_test, logreg_pred))\n\nprint(\"\\nRandom Forest Classification Report:\")\nprint(classification_report(y_test, rf_pred))\n\n# Print confusion matrices\nprint(\"\\nLogistic Regression Confusion Matrix:\")\nprint(confusion_matrix(y_test, logreg_pred))\n\nprint(\"\\nRandom Forest Confusion Matrix:\")\nprint(confusion_matrix(y_test, rf_pred))","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"90a2d7f7-cda1-40c4-8e69-6bfed5bee317","outputs":[{"output_type":"stream","name":"stdout","text":"Logistic Regression Accuracy: 0.79\nRandom Forest Accuracy: 0.79\nHigher Accuracy Model: RandomForest\n\nLogistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.79      1.00      0.88      1027\n           1       0.00      0.00      0.00       273\n\n    accuracy                           0.79      1300\n   macro avg       0.40      0.50      0.44      1300\nweighted avg       0.62      0.79      0.70      1300\n\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.79      1.00      0.88      1027\n           1       0.00      0.00      0.00       273\n\n    accuracy                           0.79      1300\n   macro avg       0.40      0.50      0.44      1300\nweighted avg       0.62      0.79      0.70      1300\n\n\nLogistic Regression Confusion Matrix:\n[[1027    0]\n [ 273    0]]\n\nRandom Forest Confusion Matrix:\n[[1027    0]\n [ 273    0]]\n"}],"execution_count":10},{"source":"# Their solution\n(I like mine better)","metadata":{},"cell_type":"markdown","id":"b555b2ae-d04e-4ffd-ace0-f6be77e8ac73"},{"source":"# Import required libraries and methods/functions\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \n# OneHotEncoder is not needed if using pd.get_dummies()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load data\ntelco_demog = pd.read_csv('telecom_demographics.csv')\ntelco_usage = pd.read_csv('telecom_usage.csv')\n\n# Join data\nchurn_df = telco_demog.merge(telco_usage, on='customer_id')\n\n# Identify churn rate\nchurn_rate = churn_df['churn'].value_counts() / len(churn_df)\nprint(churn_rate)\n\n# Identify categorical variables\nprint(churn_df.info())\n\n# One Hot Encoding for categorical variables\nchurn_df = pd.get_dummies(churn_df, columns=['telecom_partner', 'gender', 'state', 'city', 'registration_event'])\n\n# Feature Scaling\nscaler = StandardScaler()\n\n# 'customer_id' is not a feature\nfeatures = churn_df.drop(['customer_id', 'churn'], axis=1)\nfeatures_scaled = scaler.fit_transform(features)\n\n# Target variable\ntarget = churn_df['churn']\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n\n# Instantiate the Logistic Regression\nlogreg = LogisticRegression(random_state=42)\nlogreg.fit(X_train, y_train)\n\n# Logistic Regression predictions\nlogreg_pred = logreg.predict(X_test)\n\n# Logistic Regression evaluation\nprint(confusion_matrix(y_test, logreg_pred))\nprint(classification_report(y_test, logreg_pred))\n\n# Instantiate the Random Forest model\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\n# Random Forest predictions\nrf_pred = rf.predict(X_test)\n\n# Random Forest evaluation\nprint(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))\n\n# Which accuracy score is higher? Ridge or RandomForest\nhigher_accuracy = \"RandomForest\"","metadata":{"executionCancelledAt":null,"executionTime":4295,"lastExecutedAt":1716398018121,"lastExecutedByKernel":"d11e2687-deec-4ff5-8037-fb6d23870ea2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries and methods/functions\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \n# OneHotEncoder is not needed if using pd.get_dummies()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Load data\ntelco_demog = pd.read_csv('telecom_demographics.csv')\ntelco_usage = pd.read_csv('telecom_usage.csv')\n\n# Join data\nchurn_df = telco_demog.merge(telco_usage, on='customer_id')\n\n# Identify churn rate\nchurn_rate = churn_df['churn'].value_counts() / len(churn_df)\nprint(churn_rate)\n\n# Identify categorical variables\nprint(churn_df.info())\n\n# One Hot Encoding for categorical variables\nchurn_df = pd.get_dummies(churn_df, columns=['telecom_partner', 'gender', 'state', 'city', 'registration_event'])\n\n# Feature Scaling\nscaler = StandardScaler()\n\n# 'customer_id' is not a feature\nfeatures = churn_df.drop(['customer_id', 'churn'], axis=1)\nfeatures_scaled = scaler.fit_transform(features)\n\n# Target variable\ntarget = churn_df['churn']\n\n# Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n\n# Instantiate the Logistic Regression\nlogreg = LogisticRegression(random_state=42)\nlogreg.fit(X_train, y_train)\n\n# Logistic Regression predictions\nlogreg_pred = logreg.predict(X_test)\n\n# Logistic Regression evaluation\nprint(confusion_matrix(y_test, logreg_pred))\nprint(classification_report(y_test, logreg_pred))\n\n# Instantiate the Random Forest model\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\n# Random Forest predictions\nrf_pred = rf.predict(X_test)\n\n# Random Forest evaluation\nprint(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))\n\n# Which accuracy score is higher? Ridge or RandomForest\nhigher_accuracy = \"RandomForest\"","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"d1e64a26-6f1a-4747-ad3c-60836512fc3a","outputs":[{"output_type":"stream","name":"stdout","text":"0    0.799538\n1    0.200462\nName: churn, dtype: float64\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 6500 entries, 0 to 6499\nData columns (total 14 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   customer_id         6500 non-null   int64 \n 1   telecom_partner     6500 non-null   object\n 2   gender              6500 non-null   object\n 3   age                 6500 non-null   int64 \n 4   state               6500 non-null   object\n 5   city                6500 non-null   object\n 6   pincode             6500 non-null   int64 \n 7   registration_event  6500 non-null   object\n 8   num_dependents      6500 non-null   int64 \n 9   estimated_salary    6500 non-null   int64 \n 10  calls_made          6500 non-null   int64 \n 11  sms_sent            6500 non-null   int64 \n 12  data_used           6500 non-null   int64 \n 13  churn               6500 non-null   int64 \ndtypes: int64(9), object(5)\nmemory usage: 761.7+ KB\nNone\n[[920 107]\n [245  28]]\n              precision    recall  f1-score   support\n\n           0       0.79      0.90      0.84      1027\n           1       0.21      0.10      0.14       273\n\n    accuracy                           0.73      1300\n   macro avg       0.50      0.50      0.49      1300\nweighted avg       0.67      0.73      0.69      1300\n\n[[1026    1]\n [ 273    0]]\n              precision    recall  f1-score   support\n\n           0       0.79      1.00      0.88      1027\n           1       0.00      0.00      0.00       273\n\n    accuracy                           0.79      1300\n   macro avg       0.39      0.50      0.44      1300\nweighted avg       0.62      0.79      0.70      1300\n\n"}],"execution_count":11},{"source":"## Comparison Summary\nMy Code:\n- Uses StandardScaler and OneHotEncoder from sklearn.preprocessing for scaling and encoding.\n- Converts registration_event to a numerical value by calculating the number of days since the earliest date.\n- Merges data using pd.merge.\n- Defines a custom padding function for handling sequence lengths.\n- Splits data into training and testing sets using train_test_split.\n- Trains both LogisticRegression and RandomForestClassifier models and compares their accuracy.\n- Uses classification_report and confusion_matrix for model evaluation.\n\nProposed Solution:\n- Uses pd.get_dummies for one-hot encoding instead of OneHotEncoder.\n- Merges data using pd.merge.\n- Scales features using StandardScaler.\n- Splits data into training and testing sets using train_test_split.\n- Trains both LogisticRegression and RandomForestClassifier models and compares their accuracy.\n- Uses classification_report and confusion_matrix for model evaluation.","metadata":{},"cell_type":"markdown","id":"ea9b0221-6cd8-4967-9c68-c99db4444290"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}